# 5. Writing the first use case

## Setup

A use case is an operation your software performs for the benefit of its user. In this case, that operation is: listing Events happening near a location. We'll call it `ListNearbyEvents`. But where to put it?

Looking back at our architecture, we see the use case falls under "Business Logic", so maybe we should have a `Partymapper\Logic` namespace and put the class there.

![Architecture overview]()

But, before creating any files, let's think about our use case. What's its algorithm?

1. Receive geographic coordinates
2. Ask the `EventsRepository` for a list of Events 
3. Return the list, sorted by distance

That's not very complicated at all. It's so simple, I'd say we should skip the `Logic` namespace altogether and just have the Command Handler take care of it.

A very high percentage of any application is CRUD, which gets very repetitive very fast, so the simpler we make things the better. Let's make our Handlers part of the Business Logic layer on our onion, rather than having them sit outside all useless just piping data.

Use case handler class:  
`Partymapper\Handler\ListNearbyEventsHandler`

We'll also need a Command:  
`Partymapper\Command\ListNearbyEvents`

I like to keep the suffix out of my command classes for two reasons:

* Their purpose is clear enough as it is.
* Because it's the class your client code will use, it's nice to keep it short.

## Coding time

### A word on test-driven development

Test-driven development has many well known advantages over the "code first, test eventually" approach, namely:

- Tests provide a specification for input and output, which doubles as documentation
- Green tests mean your specification is satisfied and your work is done
- Observing the red-green-refactor cycle minimizes wasted effort
- High code coverage is practically assured

If you have no idea how to approach a problem, explore your options first. Code a few experiments, see what works and what doesn't, then throw it all in the trash, write a test and re-implement correctly. If you already know exactly what you need to do, your test will be easy to write. In both these scenarios, you'll probably end up with black-box testing: specify input, specify output, assert your code returns the expected output for the given input. And that's fine.

But there's an intermediate stage there, where something funny happens. When you kind of know what the approach should be but are still fuzzy on the details, TDD can help because it's a great design tool.

Here's a traditional test:

```
Load some Event fixtures
Initialize my Logic with its dependencies
Hey Logic! Here are my coordinates, give me Events
These Events look okay, test passes!
```

You set up an environment with known characteristics, call your logic with known input and assert that output matches expectations. This is great because you're free to refactor the inner workings of your logic at will. 

Trouble is, at the design stage when you're still not sure how your code should be laid out, total freedom is fertile ground for quick and dirty hacks that result in technical debt and maintenance nightmares.

By effectively employing test doubles, you can write your tests like this instead:

```
Load some Event fixtures
My Logic is initialized with a Repository that expects to be called with arguments A and B, and then returns collection X
Hey Logic! Here are coordinates A and B, give me Events
I know my Logic is supposed to return a collection straight from the repository. These Events look like collection X, so test passes!
```

A couple things strike many people as odd with this approach:

- You're testing **how** the code works, not just its output
- It's much harder to refactor

They are valid concerns. What they fail to consider is that this test doesn't appear all in one go. If it did, it's be pointless. Instead, remember, this is about design. There's a process to design.

...
